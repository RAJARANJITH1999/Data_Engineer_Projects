{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db5b22e0-e52d-4ad4-ad68-51db2f8a459c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+--------+------------+----------------+--------------+--------------+\n|transaction_id|customer_id|product_id|quantity|total_amount|transaction_date|payment_method|    store_type|\n+--------------+-----------+----------+--------+------------+----------------+--------------+--------------+\n|     TRX000001|        802|       425|       1|       363.4|      2020-07-27|    Debit Card|Physical Store|\n|     TRX000002|        858|       280|       6|      758.18|      2022-08-10|   Credit Card|Physical Store|\n|     TRX000003|        658|       694|       9|      748.66|      2020-05-22| Bank Transfer|        Online|\n|     TRX000004|        516|       930|       4|      933.78|            NULL| Bank Transfer|Physical Store|\n|     TRX000005|        368|       104|      10|      137.28|      2022-06-24|        PayPal|Physical Store|\n|     TRX000006|        606|       409|       6|      556.88|      2020-10-14|   Credit Card|        Online|\n|     TRX000007|        535|       487|       7|      246.29|      2023-08-19|   Credit Card|Physical Store|\n|     TRX000008|         87|       630|       8|     -416.82|      2020-04-18|   Credit Card|Physical Store|\n|     TRX000009|        383|       617|      10|      405.05|      2021-02-13|          Cash|Physical Store|\n|     TRX000010|        725|       938|       1|      410.03|      2020-03-18|        PayPal|        Online|\n+--------------+-----------+----------+--------+------------+----------------+--------------+--------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "filepath = 'dbfs:/FileStore/Global_Retail/Bronze_Layer/Transaction/transaction_snappy.parquet'\n",
    "df = spark.read \\\n",
    "     .format('parquet') \\\n",
    "     .load(filepath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92111509-69c2-46dc-b88e-788cbc097cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- transaction_id: string (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- transaction_date: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- store_type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21c2cce-92a3-496c-92ee-dcb47abb4724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "new_df = df.withColumn(\"transaction_date\",to_timestamp(col('transaction_date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ab15c3-b365-4e11-9ca6-aae8ffbecd1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- transaction_id: string (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- transaction_date: timestamp (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- store_type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dbfe1ad-82cb-4df6-b861-1dfced3b3358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+--------+------------+-------------------+--------------+--------------+--------------------+\n|transaction_id|customer_id|product_id|quantity|total_amount|   transaction_date|payment_method|    store_type| ingestion_timestamp|\n+--------------+-----------+----------+--------+------------+-------------------+--------------+--------------+--------------------+\n|     TRX000001|        802|       425|       1|       363.4|2020-07-27 00:00:00|    Debit Card|Physical Store|2024-10-08 23:52:...|\n|     TRX000002|        858|       280|       6|      758.18|2022-08-10 00:00:00|   Credit Card|Physical Store|2024-10-08 23:52:...|\n|     TRX000003|        658|       694|       9|      748.66|2020-05-22 00:00:00| Bank Transfer|        Online|2024-10-08 23:52:...|\n|     TRX000004|        516|       930|       4|      933.78|               NULL| Bank Transfer|Physical Store|2024-10-08 23:52:...|\n|     TRX000005|        368|       104|      10|      137.28|2022-06-24 00:00:00|        PayPal|Physical Store|2024-10-08 23:52:...|\n|     TRX000006|        606|       409|       6|      556.88|2020-10-14 00:00:00|   Credit Card|        Online|2024-10-08 23:52:...|\n|     TRX000007|        535|       487|       7|      246.29|2023-08-19 00:00:00|   Credit Card|Physical Store|2024-10-08 23:52:...|\n|     TRX000008|         87|       630|       8|     -416.82|2020-04-18 00:00:00|   Credit Card|Physical Store|2024-10-08 23:52:...|\n|     TRX000009|        383|       617|      10|      405.05|2021-02-13 00:00:00|          Cash|Physical Store|2024-10-08 23:52:...|\n|     TRX000010|        725|       938|       1|      410.03|2020-03-18 00:00:00|        PayPal|        Online|2024-10-08 23:52:...|\n+--------------+-----------+----------+--------+------------+-------------------+--------------+--------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "final_df = new_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "final_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306182e0-b2cf-4e15-a104-de94093edaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#saving as a Table now in Delta Table\n",
    "\n",
    "spark.sql(\"use globalretail_bronze\")\n",
    "\n",
    "final_df.write \\\n",
    ".format('delta') \\\n",
    ".mode('append') \\\n",
    ".saveAsTable('bronze_transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1c80b3-84f1-4698-a578-61ea10a5c865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1387934050476918>, line 4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m archive \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/Global_Retail/Bronze_Layer/Transaction/Archive/\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m path \u001B[38;5;241m=\u001B[39m archive \u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 4\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmv(filepath, path)\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:364\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf_with_exception_handling\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    363\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 364\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    365\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    367\u001B[0m         \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mExecutionError\u001B[39;00m(\u001B[38;5;167;01mException\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:419\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.mv\u001B[0;34m(self, source, dest, recurse)\u001B[0m\n",
       "\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_return(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfsutils_parallel\u001B[38;5;241m.\u001B[39mmvParallel(source, dest))\n",
       "\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_return(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfsutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmv\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecurse\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mUnsupportedOperationException\u001B[0m: com.databricks.backend.daemon.data.client.DBFSV1.resolvePathOnPhysicalStorage(path: Path)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnsupportedOperationException",
        "evalue": "com.databricks.backend.daemon.data.client.DBFSV1.resolvePathOnPhysicalStorage(path: Path)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>UnsupportedOperationException</span>: com.databricks.backend.daemon.data.client.DBFSV1.resolvePathOnPhysicalStorage(path: Path)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": null,
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)",
        "File \u001B[0;32m<command-1387934050476918>, line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m archive \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/Global_Retail/Bronze_Layer/Transaction/Archive/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m path \u001B[38;5;241m=\u001B[39m archive \u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmv(filepath, path)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(path)\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:364\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf_with_exception_handling\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    363\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 364\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    367\u001B[0m         \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mExecutionError\u001B[39;00m(\u001B[38;5;167;01mException\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:419\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.mv\u001B[0;34m(self, source, dest, recurse)\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_return(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfsutils_parallel\u001B[38;5;241m.\u001B[39mmvParallel(source, dest))\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_return(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfsutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmv\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecurse\u001B[49m\u001B[43m)\u001B[49m)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mUnsupportedOperationException\u001B[0m: com.databricks.backend.daemon.data.client.DBFSV1.resolvePathOnPhysicalStorage(path: Path)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "archive = 'dbfs:/FileStore/Global_Retail/Bronze_Layer/Transaction/Archive/'\n",
    "path = archive +'_'+ datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "dbutils.fs.mv(filepath, path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fafd2d36-0aca-4653-aad5-68a3ef1b404a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Layer_Transaction_Load",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
